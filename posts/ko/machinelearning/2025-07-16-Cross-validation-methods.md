---
layout: post
title: "Cross-validation methods"
date: 2025-07-16
categories: [Machine Learning, Data Science, Model Evaluation]
tags: [cross validation, model validation, k-fold, time series, LOOCV, stratified kfold, group kfold, rolling window, expanding window, blocked cv, sklearn, python, model selection, hold out, panel data]
math: true
mermaid: true
---

## í•™ìŠµ/í‰ê°€ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ **hold-out** ë°©ì‹

<p align="center">
  <img src="/assets/images/machinelearning/holdout.webp" width="600" alt="holdout">
  <a href="https://medium.com/@hahahumble/cross-validation-clearly-explained-in-5-graphs-9b83067bc696">Image Source</a>
</p>

```python
import numpy as np
from sklearn.model_selection import train_test_split

X = np.arange(15)
X_train, X_test = train_test_split(X, test_size=0.2, random_state=123)
print("TRAIN:", X_train)
print("TEST:", X_test)

# TRAIN: [ 0  5  9  8 11  3  1  6 12  2 13 14]
# TEST: [ 7 10  4]
```

> - Hold-out ë°©ì‹ì€ ë°ì´í„° ë¶„í• ì— ë”°ë¼ ì„±ëŠ¥ì´ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë¶ˆì•ˆì •í•˜ë‹¤.  
> - Cross-validationì€ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë²ˆ ë‚˜ëˆ„ì–´ í•™ìŠµí•¨ìœ¼ë¡œì¨ ë” ì•ˆì •ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ í‰ê°€ë¥¼ ê°€ëŠ¥ì¼€ í•œë‹¤.

## 1. Cross-sectional data ì—ì„œ Cross-validation methods

### 1-1. **Leave-One-Out CV (LOOCV)**

- í•˜ë‚˜ì˜ ë°ì´í„°ë¥¼ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ, ë‚˜ë¨¸ì§€ë¥¼ í•™ìŠµì…‹ìœ¼ë¡œ ì‚¬ìš©. ì´ ê³¼ì •ì„ ëª¨ë“  ë°ì´í„°ì— ë°˜ë³µ
- ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµ ëŠ¥ë ¥ì„ ê·¹ëŒ€í™”í•˜ê³  ì‹¶ì„ ë•Œ

<p align="center">
  <img src="/assets/images/machinelearning/LOOCV.webp" width="600" alt="LOOCV">
  <a href="https://medium.com/@hahahumble/cross-validation-clearly-explained-in-5-graphs-9b83067bc696">Image Source</a>
</p>

```python
import numpy as np
from sklearn.model_selection import LeaveOneOut

X = np.arange(5)

for train_idx, test_idx in LeaveOneOut().split(X):
    print("TRAIN:", train_idx, "/ Values:", X[train_idx])
    print("TEST: ", test_idx,  "/ Value: ", X[test_idx])
    print()

# TRAIN: [1 2 3 4] / Values: [1 2 3 4]
# TEST:  [0] / Value:  [0]

# TRAIN: [0 2 3 4] / Values: [0 2 3 4]
# TEST:  [1] / Value:  [1]

# TRAIN: [0 1 3 4] / Values: [0 1 3 4]
# TEST:  [2] / Value:  [2]

# TRAIN: [0 1 2 4] / Values: [0 1 2 4]
# TEST:  [3] / Value:  [3]

# TRAIN: [0 1 2 3] / Values: [0 1 2 3]
# TEST:  [4] / Value:  [4]
```

### 1-2. **Standard K-Fold CV**

- ë°ì´í„°ë¥¼ kê°œì˜ ë™ì¼í•œ í¬ê¸°ì˜ í´ë“œë¡œ ë‚˜ëˆˆ í›„ ê° í´ë“œë¥¼ í•œ ë²ˆì”© í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ì‚¬ìš©
- ê· í˜• ì¡íŒ ë°ì´í„°ì…‹ì—ì„œ ë³´í¸ì ìœ¼ë¡œ ì‚¬ìš©

<p align="center">
  <img src="/assets/images/machinelearning/kfold.png" width="600" alt="kfold">
  <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py">Image Source</a>
</p>

```python
import numpy as np
from sklearn.model_selection import KFold

X = np.arange(15)
kf = KFold(n_splits=3, shuffle=True, random_state=123)
for train_idx, test_idx in kf.split(X):
    print("TRAIN:", train_idx, "TEST:", test_idx)

# TRAIN: [ 1  2  3  6  8  9 11 12 13 14] TEST: [ 0  4  5  7 10]
# TRAIN: [ 0  2  4  5  6  7 10 12 13 14] TEST: [ 1  3  8  9 11]
# TRAIN: [ 0  1  3  4  5  7  8  9 10 11] TEST: [ 2  6 12 13 14]    
```

### 1-3. **Stratified K-Fold CV**

- ì›ë˜ ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ê° í´ë“œì—ì„œë„ ë™ì¼í•˜ê²Œ ìœ ì§€í•¨
- í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ìˆëŠ” ë¶„ë¥˜ ë¬¸ì œì—ì„œ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ìœ ì§€í•˜ê³ ì í•  ë•Œ

<p align="center">
  <img src="/assets/images/machinelearning/stratifiedkfold.png" width="600" alt="stratifiedkfold">
  <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py">Image Source</a>
</p>

```python
import numpy as np
from collections import Counter
from sklearn.model_selection import StratifiedKFold

X = np.arange(15)
y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)

for train_idx, test_idx in skf.split(X, y):
    y_train = np.array(y)[train_idx]
    y_test = np.array(y)[test_idx]

    print("TRAIN:", train_idx, " / Class count:", dict(Counter(y_train)))
    print("TEST: ", test_idx,  " / Class count:", dict(Counter(y_test)))
    print()

# TRAIN: [ 1  2  4  5  6  7  9 10 11 13]  / Class count: {0: 3, 1: 7}
# TEST:  [ 0  3  8 12 14]  / Class count: {0: 2, 1: 3}

# TRAIN: [ 0  2  3  5  6  8 10 11 12 14]  / Class count: {0: 3, 1: 7}
# TEST:  [ 1  4  7  9 13]  / Class count: {0: 2, 1: 3}

# TRAIN: [ 0  1  3  4  7  8  9 12 13 14]  / Class count: {0: 4, 1: 6}
# TEST:  [ 2  5  6 10 11]  / Class count: {0: 1, 1: 4}
```

### 1-4. **Group K-Fold CV**

- ë™ì¼ ê·¸ë£¹ì´ í•™ìŠµì…‹ê³¼ í…ŒìŠ¤íŠ¸ì…‹ì— ë™ì‹œì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ ë‚˜ëˆ”
- ë°ì´í„° ìƒ˜í”Œ ê°„ ë…ë¦½ì„±ì´ ì—†ëŠ” ê²½ìš° ê·¸ë£¹ ë‹¨ìœ„ ì„±ëŠ¥ í‰ê°€ì— ì í•©

<p align="center">
  <img src="/assets/images/machinelearning/groupkfold.png" width="600" alt="groupkfold">
  <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py">Image Source</a>
</p>

```python
import numpy as np
from sklearn.model_selection import GroupKFold

X = np.arange(10)
groups = np.array([1, 1, 1, 2, 2, 3, 3, 3, 4, 4])  # ê°™ì€ ê·¸ë£¹ë¼ë¦¬ëŠ” ë¶„ë¦¬ë¨
gkf = GroupKFold(n_splits=3, shuffle=True, random_state=123)

for train_idx, test_idx in gkf.split(X, groups=groups):
    print("TRAIN:", train_idx, " / Group:", np.unique(groups[train_idx]))
    print("TEST: ", test_idx,  " / Group:", np.unique(groups[test_idx]))
    print()

# TRAIN: [3 4 5 6 7]  / Class: [2 3]
# TEST:  [0 1 2 8 9]  / Class: [1 4]

# TRAIN: [0 1 2 5 6 7 8 9]  / Class: [1 3 4]
# TEST:  [3 4]  / Class: [2]

# TRAIN: [0 1 2 3 4 8 9]  / Class: [1 2 4]
# TEST:  [5 6 7]  / Class: [3]
```

## 2. Time series data ì—ì„œ Cross-validation methods

look-ahead bias ë°©ì§€ë¥¼ ìœ„í•´ ì¼ë°˜ì ì¸ CV method ì‚¬ìš© ë¶ˆê°€

### 2-1.  **Rolling Window CV**

<p align="center">
  <img src="/assets/images/machinelearning/slidingwindowCV.png" width="600" alt="slidingwindowCV">
  <a href="https://www.kaggle.com/code/cworsnup/backtesting-cross-validation-for-timeseries/notebook">Image Source</a>
</p>

```python
import numpy as np
from sklearn.model_selection import TimeSeriesSplit

x = np.arange(15)
cv = TimeSeriesSplit(max_train_size=3, test_size=1)
for train_index, test_index in cv.split(x):
    print("TRAIN:", train_index, "TEST:", test_index)

# TRAIN: [7 8 9] TEST: [10]
# TRAIN: [8 9 10] TEST: [11]
# TRAIN: [9 10 11] TEST: [12]
# TRAIN: [10 11 12] TEST: [13]
# TRAIN: [11 12 13] TEST: [14]
```

### 2-2.**Expanding Window CV**

<p align="center">
  <img src="/assets/images/machinelearning/expandingwindowCV.png" width="600" alt="expandingwindowCV">
  <a href="https://www.kaggle.com/code/cworsnup/backtesting-cross-validation-for-timeseries/notebook">Image Source</a>
</p>

```python
import numpy as np
from sklearn.model_selection import TimeSeriesSplit

x = np.arange(15)
cv = TimeSeriesSplit(n_splits=8, test_size=1)
for train_index, test_index in cv.split(x):
    print("TRAIN:", train_index, "TEST:", test_index)

# TRAIN: [0 1 2 3 4 5 6] TEST: [7]
# TRAIN: [0 1 2 3 4 5 6 7] TEST: [8]
# TRAIN: [0 1 2 3 4 5 6 7 8] TEST: [9]
# TRAIN: [0 1 2 3 4 5 6 7 8 9] TEST: [10]
# TRAIN: [0 1 2 3 4 5 6 7 8 9 10] TEST: [11]
# TRAIN: [0 1 2 3 4 5 6 7 8 9 10 11] TEST: [12]
# TRAIN: [0 1 2 3 4 5 6 7 8 9 10 11 12] TEST: [13]
# TRAIN: [0 1 2 3 4 5 6 7 8 9 10 11 12 13] TEST: [14]
```

### 2-3. **Blocked CV**

<p align="center">
  <img src="/assets/images/machinelearning/blockedCV.jpg" width="600" alt="blockedCV">
  <a href="https://www.packtpub.com/en-us/learning/how-to-tutorials/cross-validation-strategies-for-time-series-forecasting-tutorial/">Image Source</a>
</p>

```python
import numpy as np

# Source code from: https://hub.packtpub.com/cross-validation-strategies-for-time-series-forecasting-tutorial/
class BlockedTimeSeriesSplit():
    def __init__(self, n_splits):
        # Define 
        self.n_splits = n_splits

    def get_n_splits(self, X, y, groups):
        return self.n_splits

    def split(self, X, y=None, groups=None):
        # Split data using a blocked strategy
        # and return indices of where to split
        n_samples = len(X)
        k_fold_size = n_samples // self.n_splits
        indices = np.arange(n_samples)

        margin = 0
        for i in range(self.n_splits):
            start = i * k_fold_size
            stop = start + k_fold_size
            mid = int(0.8 * (stop - start)) + start
            yield indices[start: mid], indices[mid + margin: stop]



btscv = BlockedTimeSeriesSplit(n_splits=3)
X = np.arange(15)

for train_index, test_index in btscv.split(X):
    print("TRAIN:", train_index, "TEST:", test_index)

# TRAIN: [0 1 2 3] TEST: [4]
# TRAIN: [5 6 7 8] TEST: [9]
# TRAIN: [10 11 12 13] TEST: [14]
```

## 3. íŒ¨ë„ í…Œì´í„°ì—ì„œì˜ CV  

[Time-series grouped cross-validation](https://datascience.stackexchange.com/questions/77684/time-series-grouped-cross-validation?answertab=modifieddesc#tab-top)

---

## ğŸ”— ì°¸ê³  ìë£Œ  

- [Time-series grouped cross-validation](https://datascience.stackexchange.com/questions/77684/time-series-grouped-cross-validation?answertab=modifieddesc#tab-top)
- [Visualizing cross-validation behavior in scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py)
- [Cross-Validation Clearly Explained in 5 Graphs](https://medium.com/@hahahumble/cross-validation-clearly-explained-in-5-graphs-9b83067bc696)
- [The Ultimate Guide to Time Series CV](https://www.numberanalytics.com/blog/ultimate-guide-time-series-cv)
- [Time Series Analysis: Walk-Forward Validation](https://www.linkedin.com/pulse/time-series-analysis-walk-forward-validation-rafi-ahmed-4v91c)
- [Forecast evaluation for data scientists: common pitfalls and best practices](https://www.researchgate.net/publication/365969672_Forecast_evaluation_for_data_scientists_common_pitfalls_and_best_practices)
- [Backtesting - Cross-Validation for TimeSeries](https://www.kaggle.com/code/cworsnup/backtesting-cross-validation-for-timeseries/notebook)
- [Add rolling window to sklearn.model_selection.TimeSeriesSplit #22523](https://github.com/scikit-learn/scikit-learn/issues/22523)
- [Cross-Validation strategies for Time Series forecasting [Tutorial]](https://www.packtpub.com/en-us/learning/how-to-tutorials/cross-validation-strategies-for-time-series-forecasting-tutorial/)
