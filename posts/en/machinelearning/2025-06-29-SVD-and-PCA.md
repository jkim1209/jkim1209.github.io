---
layout: post
title: "SVD and PCA"
date: 2025-06-29
categories: [Machine Learning, Statistics]
tags: [singular value decomposition, principal component analysis, orthogonal matrix, gram schmidt, pseudo inverse, truncated svd, covariance matrix]
math: true
---

## 1. Eigenvalues and Eigenvectors

### (1) Definition

For an $n\times n$ matrix $A$, $\lambda$ satisfying $A\vec{v}=\lambda \vec{v}$ is called an eigenvalue, and $\vec{v}\neq\vec{0}$ is called an eigenvector.

### (2) Derivation

$1.$ Find eigenvalue $\lambda$ satisfying $Det(A-\lambda I)=0$.

$2.$ Substitute the obtained eigenvalue into $(A-\lambda I)\vec{v}=0$ to find eigenvector $\vec{v}$.

$3.$ For eigenvalue $\lambda$ and eigenvector $\vec{v}$ of $n\times n$ matrix $A$, $E_\lambda=Span(\vec{v})$ is called the eigenspace.

### (3) Theorems

$1.$ The eigenvalues of a triangular matrix are its diagonal elements.

$2.$ Let the distinct eigenvalues of an $n\times n$ matrix be $\lambda_1, \lambda_2, \cdots, \lambda_k$, and let the eigenvectors corresponding to each eigenvalue be $\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k$. Then $\{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k \}$ is linearly independent.

---

## 2. Gram-Schmidt Orthogonalization

### 1) Orthogonality of Two Vectors

#### (1) Definition

For elements $\vec{u},\vec{v}$ in an inner product space $V$, $\vec{u}$ and $\vec{v}$ are orthogonal means their inner product is $0$.
That is, $\vec{u}\perp \vec{v} \Leftrightarrow \ <\vec{u},\vec{v}>=0.$

### 2) Norm of a Vector

#### (1) Definition

For element $\vec{u}$ in an inner product space $V$, the norm of vector $\vec{u}$ is defined as $\|\vec{u}\|=\sqrt{<\vec{u},\vec{u}>}$.

#### (2) Theorems

$1.\ \|\vec{u}+\vec{v}\|^2=\|\vec{u}\|^2+\|\vec{v}\|^2+2<\vec{u},\vec{v}>$

$2.\ \|\vec{u}+\vec{v}\|^2+\|\vec{u}-\vec{v}\|^2=2\|\vec{u}\|^2+2\|\vec{v}\|^2$

$3.\ <\vec{u},\vec{v}>=\dfrac{1}{4}\|\vec{u}+\vec{v}\|^2-\dfrac{1}{4}\|\vec{u}-\vec{v}\|^2$

$4.\ $ A necessary and sufficient condition for $\vec{u}, \vec{v}$ to be orthogonal is $\|\vec{u}+\vec{v}\|^2=\|\vec{u}\|^2+\|\vec{v}\|^2$.

$5.\ \|<\vec{u}, \vec{v}>\| \le \|\vec{u}\| \|\vec{v}\|$ (equality holds when $\vec{u}$ and $\vec{v}$ are parallel)

$6.\ \|\vec{u}+\vec{v}\| \le \|\vec{u}\| + \|\vec{v}\|$

> - The dot product $\vec{u}\cdot \vec{v}=\vec{u}^T\vec{v}$ defined in $\mathbb{R}^n$ is also an inner product.
> - Therefore, orthogonality can be discussed for the dot product defined in $\mathbb{R}^n$.
> - The following content also holds for general inner product spaces, but is described limited to the dot product defined in $\mathbb{R}^n$.

### 3) Orthogonal Set

#### (1) Definition

For $\vec{v}_1,\ \vec{v}_2,\ \cdots,\ \vec{v}_k \in \mathbb{R}^n$ with respect to the dot product defined in $\mathbb{R}^n$,
$\{ \vec{v}_1,\ \vec{v}_2,\ \cdots,\ \vec{v}_k \}$ being an orthogonal set means $\vec{v}_i \cdot \vec{v}_j=0, \quad i\neq j$.

#### (2) Theorems

$1.$ For any vector $\vec{u} \in V$ in the subspace $V$ of $\mathbb{R}^n$ generated by orthogonal set $B=\{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k \}$ (i.e., $V=Span(B) \subseteq \mathbb{R}^n$),

$\quad \vec{u}=\dfrac{\vec{u} \cdot \vec{v}_1}{\vec{v}_1 \cdot \vec{v}_1}\vec{v}_1   + \dfrac{\vec{u} \cdot \vec{v}_2}{\vec{v}_2 \cdot \vec{v}_2}\vec{v}_2  +  \cdots  + \dfrac{\vec{u} \cdot \vec{v}_k}{\vec{v}_k \cdot \vec{v}_k}\vec{v}_k$ can be represented.

$2.$ If orthogonal set $\{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k \}$ does not contain the zero vector, then $\{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k \}$ is linearly independent.

### 4) Orthogonal Basis

#### (1) Definition

For subspace $V$ of $\mathbb{R}^n$, if $B=\{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k \}$ is both ①a basis of $V$ and ②an orthogonal set, then $B$ is an orthogonal basis of $V$.

### 5) Orthonormal Set

#### (1) Definition

$\{ \vec{v}_1,\ \vec{v}_2,\ \cdots,\ \vec{v}_k \}$ being an orthonormal set means
① $<\vec{v}_i, \vec{v}_j>=0, \quad i\neq j$
② $\|\vec{v}_i\|=1, \quad \forall{i}$

> Note: The standard basis of $\mathbb{R}^n$ is an orthonormal set.

### 6) Orthonormal Basis

#### (1) Definition

For subspace $V$ of $\mathbb{R}^n$, if $B=\{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k \}$ is both ①a basis of $V$ and ②an orthonormal set, then $B$ is an orthogonal basis of $W$.

#### (2) Theorems

$1.$ For any vector $\vec{u} \in V$ in the subspace $V$ of $\mathbb{R}^n$ generated by orthonormal set $B=\{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k \}$,
$\quad \vec{u} \ = \ (\vec{u} \cdot \vec{v}_1)\vec{v}_1 \ + \ (\vec{u} \cdot \vec{v}_2)\vec{v}_2 \ + \ \cdots \ + \ (\vec{u} \cdot \vec{v}_k)\vec{v}_k$ can be represented.

$2.$ If the column vectors of $m\times n \ (m\ge n)$ matrix $A$ are orthonormal, then $A^TA=I_n$.

### 7) Orthogonal Matrix

#### (1) Definition

$A$ being an orthogonal matrix means ① $A: n \times n$ matrix, ② The column vectors of $A$ form an **orthonormal** set.

#### (2) Theorems

$1.$ A necessary and sufficient condition for square matrix $A$ to be an orthogonal matrix is $A^TA=I$, i.e., $A^T=A^{-1}$.
$2.$ From $A^T=A^{-1}$, we also have $AA^T=I$, which means that if square matrix $A$ is an orthogonal matrix, the set of row vectors of $A$ is also an orthonormal set.

### 8) Orthogonal Projection

#### (1) Derivation

##### [1] Orthogonal Projection in $\mathbb{R}^2$

For linearly independent $\vec{u},\vec{v} \in \mathbb{R}^2$ (i.e., $\vec{u},\vec{v}$ form a basis of $\mathbb{R}^2$)

① $\vec{u_{pr}}=\dfrac{\vec{u} \cdot \vec{v}}{\vec{v} \cdot \vec{v}}\vec{v}\qquad$: Orthogonal projection of $\vec{u}$ onto $\vec{v}$ (vector projection)

② $\vec{u_c}=\vec{u}-\vec{u_{pr}} \qquad$ : Vector component of $\vec{u}$ orthogonal to $\vec{v}$

(i) $\vec{u_{pr}}+\vec{u_c}=\vec{u} \qquad$(ii) $\vec{u_{pr}} \parallel \vec{v} \qquad$ (iii) $\vec{u_c} \perp \vec{v}$

<center><img src='/assets/images/machinelearning/SVD_orthogonal_projection_r2.jpg' width = 600 alt="SVD_orthogonal_projection_r2"></center>

##### [2] Orthogonal Projection in $\mathbb{R}^3$

For linearly independent $\vec{u},\vec{v}_1,\vec{v}_2  \in \mathbb{R}^3$, orthogonal vectors $\vec{v}_1,\vec{v}_2$

① $\vec{u_{1}}=\dfrac{\vec{u} \cdot \vec{v}_1}{\vec{v}_1 \cdot \vec{v}_1}\vec{v}_1\qquad$: Orthogonal projection of $\vec{u}$ onto $\vec{v}_1$

② $\vec{u_{2}}=\dfrac{\vec{u} \cdot \vec{v}_2}{\vec{v}_2 \cdot \vec{v}_2}\vec{v}_2\qquad$: Orthogonal projection of $\vec{u}$ onto $\vec{v}_2$

③ $\vec{u_{pr}}= \vec{u_{1}} + \vec{u_{2}} = \dfrac{\vec{u} \cdot \vec{v}_1}{\vec{v}_1 \cdot \vec{v}_1}\vec{v}_1 + \dfrac{\vec{u} \cdot \vec{v}_2}{\vec{v}_2 \cdot \vec{v}_2}\vec{v}_2 \qquad$ : Orthogonal projection of $\vec{u}$ onto $Span(\vec{v}_1,\vec{v}_2)$

④ $\vec{u_c}=\vec{u}-\vec{u_{pr}}=\vec{u}-\dfrac{\vec{u} \cdot \vec{v}_1}{\vec{v}_1 \cdot \vec{v}_1}\vec{v}_1 - \dfrac{\vec{u} \cdot \vec{v}_2}{\vec{v}_2 \cdot \vec{v}_2}\vec{v}_2 \qquad$ : Vector component of $\vec{u}$ orthogonal to $Span(\vec{v}_1,\vec{v}_2)$

<center><img src='/assets/images/machinelearning/SVD_orthogonal_projection_r3.jpg' width = 600 alt="SVD_orthogonal_projection_r3"></center>

##### [3] Orthogonal Projection in $\mathbb{R}^n$

Given an orthogonal basis $B=\{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k \}$ for subspace $V$ of $\mathbb{R}^n$, for any vector $\vec{u}$ in $V$

① $\vec{u_{pr}}=\dfrac{\vec{u} \cdot \vec{v}_1}{\vec{v}_1 \cdot \vec{v}_1}\vec{v}_1   + \dfrac{\vec{u} \cdot \vec{v}_2}{\vec{v}_2 \cdot \vec{v}_2}\vec{v}_2   +   \cdots   + \dfrac{\vec{u} \cdot \vec{v}_k}{\vec{v}_k \cdot \vec{v}_k}\vec{v}_k \qquad$ : Orthogonal projection of $\vec{u}$ onto $V$

② $\vec{u_c}=\vec{u}-\vec{u_{pr}}=\vec{u}-\dfrac{\vec{u} \cdot \vec{v}_1}{\vec{v}_1 \cdot \vec{v}_1}\vec{v}_1   - \dfrac{\vec{u} \cdot \vec{v}_2}{\vec{v}_2 \cdot \vec{v}_2}\vec{v}_2   -   \cdots   - \dfrac{\vec{u} \cdot \vec{v}_k}{\vec{v}_k \cdot \vec{v}_k}\vec{v}_k \qquad$ : Vector component of $\vec{u}$ orthogonal to $V$

#### (2) Theorem: Gram-Schmidt Orthogonalization Process

For subspace $V$ of $\mathbb{R}^n$, when $B=\{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k \}$ is a basis of $V$, an orthogonal basis $C=\{ \vec{u}_1, \vec{u}_2, \cdots, \vec{u}_k \}$ and
orthonormal basis $D=\{ \vec{w}_1, \vec{w}_2, \cdots, \vec{w}_k \}$ also exist. Here, $C$ and $D$ can be obtained as follows:

$$
\vec{u}_1=\vec{v}_1
$$

$$
\vec{u}_2=\vec{v}_2-\dfrac{\vec{v}_2 \cdot \vec{u}_1}{\vec{u}_1 \cdot \vec{u}_1}\vec{u}_1
$$

$$
\vec{u}_3=\vec{v}_3-\dfrac{\vec{v}_3 \cdot \vec{v}_1}{\vec{u}_1 \cdot \vec{u}_1}\vec{u}_1-\dfrac{\vec{v}_3 \cdot \vec{u}_2}{\vec{u}_2 \cdot \vec{u}_2}\vec{u}_2
$$

$$
\cdots
$$

$$
\vec{u}_k=\vec{v}_k-\dfrac{\vec{v}_k \cdot \vec{u}_1}{\vec{u}_1 \cdot \vec{u}_1}\vec{u}_1-\dfrac{\vec{v}_k \cdot \vec{u}_2}{\vec{u}_2 \cdot \vec{u}_2}\vec{u}_2 - \cdots -\dfrac{\vec{v}_k \cdot \vec{u}_{k-1}}{\vec{u}_{k-1} \cdot \vec{u}_{k-1}}\vec{u}_{k-1}
$$

$$
\vec{w}_1=\dfrac{\vec{u}}{\|u_1\|}, \quad \vec{w}_2=\dfrac{\vec{u}}{\|u_2\|}, \quad \cdots, \quad \vec{w}_k=\dfrac{\vec{u}}{\|u_k\|}
$$

> ① $\vec{u}_1=\vec{v}_1\qquad$Now that we've created $\vec{u}_1$, we don't use $\vec{v}_1$ anymore.
> ② Since we want to find vector $\vec{u}_2$ orthogonal to $\vec{u}_1$,
>
> $$
> \quad \vec{u}_2=\vec{v}_{2c}=\vec{v}_2-\vec{v}_{2pr}=\vec{v}_2-\dfrac{\vec{v}_2 \cdot \vec{u}_1}{\vec{u}_1 \cdot \vec{u}_1}\vec{u}_1
> $$
>
> <center><img src='/assets/images/machinelearning/SVD_gram_schmidt_r2.jpg' width = 600 alt="SVD_gram_schmidt_r2"></center>
>
>
> $\quad$ Now that we've created $\vec{u}_2$, we don't use $\vec{v}_2$ anymore.
>
>
> ③ Since we made $\vec{u}_1$ and $\vec{u}_2$ orthogonal above, to find $\vec{u}_3$ orthogonal to $Span(\vec{u}_1,\vec{u}_2)$
>
>
> $$
> \quad \vec{u}_2=\vec{v}_{3c}=\vec{v}_3-\vec{v}_{3pr}=\vec{v}_3-\dfrac{\vec{v}_3 \cdot \vec{u}_1}{\vec{u}_1 \cdot \vec{u}_1}\vec{u}_1-\dfrac{\vec{v}_3 \cdot \vec{u}_2}{\vec{u}_2 \cdot \vec{u}_2}\vec{u}_2
> $$
>
> <center><img src='/assets/images/machinelearning/SVD_gram_schmidt_r3.jpg' width = 600 alt="SVD_gram_schmidt_r3"></center>
>
>
> $\quad$ Now that we've created $\vec{u}_3$, we don't use $\vec{v}_3$ anymore.
>
>
> ④ Repeating this,
>
> $$
> \quad \vec{u}_k=\vec{v}_k-\dfrac{\vec{v}_k \cdot \vec{u}_1}{\vec{u}_1 \cdot \vec{u}_1}\vec{u}_1-\dfrac{\vec{v}_k \cdot \vec{u}_2}{\vec{u}_2 \cdot \vec{u}_2}\vec{u}_2 -   \cdots   -\dfrac{\vec{v}_k \cdot \vec{u}_{k-1}}{\vec{u}_{k-1} \cdot \vec{u}_{k-1}}\vec{u}_{k-1}
> $$

---

## 3. Singular Value Decomposition (SVD)

#### (1) Definition

For $m\times n$ matrix $A$, it can be expressed in the form $A=U\Sigma V^T$. Here
$V: n\times n$ orthogonal matrix, $\quad \Sigma: m\times n$ matrix. In particular

$$
\Sigma=\begin{bmatrix} D & 0 \\ 0 & 0 \end{bmatrix}
, D=\begin{bmatrix} \sigma_1 & 0 & \cdots & 0 \\ 0 & \sigma_2 &\cdots & 0 \\ \cdots & \cdots & \cdots &\cdots \\ 0 & 0 & \cdots & \sigma_r\end{bmatrix}
$$

> <center><img src='/assets/images/machinelearning/SVD_SVD.webp' width = 900 alt="SVD_SVD"></center>
>
><div align="center">
>  <a href="https://towardsdatascience.com/singular-value-decomposition-svd-demystified-57fc44b802a0/" style="font-style: italic; color: #888; text-decoration: none; border-bottom: none;">Singular Value Decomposition (SVD), Demystified</a>
></div>

#### (2) Derivation

① Find symmetric matrix $A^TA$ (can be orthogonally diagonalized)

② Find eigenvalues $\lambda_1,\lambda_2,\cdots,\lambda_n$ of $A^TA$

③ Find eigenvectors corresponding to eigenvalues $\lambda_1,\lambda_2,\cdots,\lambda_n$, then convert them to **orthonormal** eigenvectors $\vec{v}_1,\vec{v}_2,\cdots,\vec{v}_n$

- Eigenvectors from different eigenvalues are guaranteed to be orthogonal, but if there are multiple eigenvectors from one eigenvalue due to multiplicity, they are not guaranteed to be orthogonal to each other, so orthogonality must be achieved through the Gram-Schmidt orthogonalization process.

④ Find singular values $\quad \sigma_1=\sqrt{\lambda_1} \quad \ge \quad \sigma_2=\sqrt{\lambda_2} \quad \ge \quad \cdots \quad \ge \quad  \sigma_n=\sqrt{\lambda_n} \quad \ge \quad 0$
$\quad$ Let $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_r > 0, \quad \sigma_{r+1}=\sigma_{r+2}=\cdots=\sigma_n=0$.

⑤ Find $\vec{u}_i=\dfrac{1}{\sigma_i}A\vec{v}_i \quad (i=1,2,\cdots,r)$

⑥ From orthonormal set $\{\vec{u}_1,\vec{u}_2,\cdots,\vec{u}_r\} \subset{\mathbb{R}^n}$, find an orthonormal basis for $\mathbb{R}^m$

$$
\{\vec{u}_1, \vec{u}_2, \cdots, \vec{u}_r,\vec{u}_{r+1}, \cdots, \vec{u}_m\}
$$

- Can be found through Gram-Schmidt orthogonalization by finding standard basis vectors of $\mathbb{R}^n$ that are linearly independent from orthonormal set $\{\vec{u}_1,\vec{u}_2,\cdots,\vec{u}_r\}$.

⑦ $U=[\vec{u}_1 \quad \vec{u}_2 \quad \cdots \quad \vec{u}_m], \qquad V=[\vec{v}_1 \quad \vec{v}_2 \quad \cdots \quad \vec{v}_n]$

#### (3) Theorems

For $m\times n$ matrix $A$, let $V, \Sigma, U$ be the singular value decomposition matrices, and let $\sigma_1, \sigma_2, \cdots, \sigma_r$ be all non-zero singular values of $A$

$$
V=\begin{bmatrix} \vec{v}_1 & \vec{v}_2 & \cdots & \vec{v}_r & | &\vec{v}_{r+1} & \vec{v}_{r+2} & \cdots & \vec{v}_n \end{bmatrix}
$$

$$
U=\begin{bmatrix} \vec{u}_1 & \vec{u}_2 & \cdots & \vec{u}_r & | &\vec{u}_{r+1} & \vec{u}_{r+2} & \cdots & \vec{u}_m \end{bmatrix}
$$

(i) $ rank(A)=r$

(ii) $\{\vec{u}_1,\vec{u}_2,\cdots,\vec{u}_r\}$ $\quad$ orthonormal basis of $\text{Col}(A)$

(iii) $\{\vec{u_{r+1}},\vec{u_{r+2}},\cdots,\vec{u}_m\}$ $\quad$ orthonormal basis of $\text{Null}(A^T)$

(iv) $\{\vec{v}_1,\vec{v}_2,\cdots,\vec{v}_r\}$ $\quad$ orthonormal basis of $\text{Row}(A)$

(v) $\{\vec{v_{r+1}},\vec{v_{r+2}},\cdots,\vec{v}_n\}$ $\quad$ orthonormal basis of $\text{Null}(A)$

#### (4) Pseudo Inverse (Moore-Penrose Inverse)

For $m\times n$ matrix $A$, let $V, \Sigma, U$ be the singular value decomposition matrices. That is, $A=U\Sigma V^T$.
Then $A^+=U\Sigma^+ V^T$ is called the pseudo inverse of $A$. Here $\Sigma^+$ is as follows:

$$
\Sigma^+=\begin{bmatrix} D^{-1} & 0 \\ 0 & 0 \end{bmatrix}_{n\times m}, \qquad

D^{-1}=\begin{bmatrix} \sigma_1^{-1} & 0 & \cdots & 0 \\ 0 & \sigma_2^{-1} &\cdots & 0 \\ \cdots & \cdots & \cdots &\cdots \\ 0 & 0 & \cdots & \sigma_r^{-1}\end{bmatrix}_{r\times r}
$$

If $A$ is an invertible matrix, then $A^{-1}=A^+$.

---

## 4. Principal Component Analysis (PCA)

### (1) Purpose

- To project high-dimensional data into lower dimensions while minimizing information loss.
- The greater the variance, the better it can explain the data.
- Therefore, the key is finding the direction (principal component) with the greatest variance in the data.

### (2) Method

Transform the coordinate axes representing existing input variables to other coordinate axes, then select only a few dimensions that best explain the data (with greatest variance) to reduce dimensions.

> **Example**
>
> **Step 1.** Two features X, Y exist
>
>
> <img src='/assets/images/machinelearning/PCA_pca1.webp' width="600" alt="PCA_pca1" style="display: block; margin: 0;">
>
> **Step 2.** Find the coordinate axis with the greatest variance and a new axis orthogonal to that axis → V1 (coordinate axis with greatest variance), V2
>
>
> <img src='/assets/images/machinelearning/PCA_pca2.webp' width="600" alt="PCA_pca2" style="display: block; margin: 0;">
>
> **Step 3.** Make V1, V2 into new coordinate axes PC1, PC2, and keep only coordinate axis PC1 that best explains the data.
>
>
> <img src='/assets/images/machinelearning/PCA_pca3.webp' width="600" alt="PCA_pca3" style="display: block; margin: 0;">
> <a href="https://statisticsbyjim.com/basics/principal-component-analysis/" style="font-style: italic; color: #888; text-decoration: none; border-bottom: none;">Principal Component Analysis Guide & Example</a>

### (2) Detailed Steps Using Formulas

#### [1] Data Normalization (Mean Removal)

Let $X$ be data with $n$ observations and $p$ features: $X \in \mathbb{R}^{n\times p}$
First, perform centering for each feature so that the mean of each feature becomes 0.
For each observation $i=1,2,\cdots,n$ and feature $j=1,2,\cdots,p$

① Calculate column-wise mean

$\mu_j = \dfrac{1}{n} \sum_{i=1}^n X_{ij}, \quad \text{for } j = 1, 2, \dots, p$

Expressed as a vector,

$\vec{\mu} = \dfrac{1}{n} X^\top \mathbf{1}_n$, $\quad \mathbf{1}_n \in \mathbb{R}^{n\times 1}$ is a column vector with all elements 1.

Therefore $\vec{\mu} = \begin{bmatrix} \mu_1 \\\ \mu_2 \\\ \vdots \\\ \mu_p \end{bmatrix} \in \mathbb{R}^{p\times 1}$ is a p-dimensional column vector.

② Remove mean

$\tilde{X_{ij}} = X_{ij} - \mu_j, \quad \text{for } i = 1, \dots, n,\ j = 1, \dots, p$

Expressed as a vector,

$\tilde{X} = X - \mathbf{1}_n \vec{\mu}^\top ,\quad \tilde{X} \in \mathbb{R}^{n\times p}$

#### [2] Perform SVD

$\tilde{X} = U \Sigma V^\top$

At this time
① $V$ is an orthogonal matrix, so the column vectors of $V$ are orthonormal, and
② From each singular value (standard deviation)
$\sigma_1=\sqrt{\lambda_1} \ge \sigma_2=\sqrt{\lambda_2} \ge \cdots \ge \sigma_n=\sqrt{\lambda_n} \ge 0$, the orthonormal vectors are arranged in corresponding order in matrix $V=[\vec{v}_1 \quad \vec{v}_2 \quad \cdots \quad \vec{v}_n]$.
$\quad$ **That is, the column vectors of $V$ are sorted in descending order by the square of singular values (=variance).**

> **Covariance Matrix and SVD**
>
> - For data centered with mean 0, the covariance matrix of matrix $\tilde{X} \in \mathbb{R}^{n\times p}$ is
> $Cov(\tilde{X}) = \frac{1}{n} \tilde{X}^\top \tilde{X}$, and eigenvalue decomposition of this covariance matrix gives
> $Cov(\tilde{X}) = V \Lambda V^\top$ ($\Lambda = diag(\lambda_1,\cdots,\lambda_p)$ : eigenvalues (variance size, descending order), $V \in \mathbb{R}^{p\times p}$ : eigenvectors corresponding to each $\lambda$).
>
> - Now substituting SVD decomposition of $\tilde{X}$, $\tilde{X} = U \Sigma V^\top$, since $U=[\vec{u}_1 \quad \vec{u}_2 \quad \cdots \quad \vec{u}_n]$ is an orthogonal matrix ($\{\vec{u}_1, \cdots,\vec{u}_n\}$ is an orthonormal set)
>
> $$
> \tilde{X}^\top \tilde{X}
> = (U \Sigma V^\top)^\top (U \Sigma V^\top)
> = V \Sigma^\top U^\top U \Sigma V^\top
> = V \Sigma^2 V^\top
> $$
>
> $\qquad \therefore$ Eigenvalues of $\tilde{X}^\top \tilde{X}$: Diagonal elements of $\Sigma^2$ (**i.e., variance**)
> $\qquad \quad$ Eigenvectors of $\tilde{X}^\top \tilde{X}$: Column vectors of $V$

#### [3] Select Principal Components

$V=[\vec{v}_1 \quad \vec{v}_2 \quad \cdots \quad \vec{v}_n] \in \mathbb{R}^{p \times p}$
Select top $k$ principal components:
$W = [\vec{v}_1 \quad \vec{v}_2 \quad \cdots \quad \vec{v}_k] \in \mathbb{R}^{p \times k}$

#### [4] Dimensionality Reduction

$Z = \tilde{X} W \in \mathbb{R}^{n \times k}$
→ This is the result of projecting original data $X$ onto the $k$-dimensional principal component space.

#### [5] Reconstruction to Original Dimensions (Optional)

$\hat{X} = Z W^\top + \bar{X} \qquad \text{where} \quad \bar{X} = \mathbf{1}_n \vec{\mu}^\top \in \mathbb{R}^{n\times p}$ (column mean vector of data, i.e., mean value before centering)
→ Reconstructed data $\hat{X} \in \mathbb{R}^{n \times p}$

> **Note: Standardization in [1] Data Normalization (mean 0, variance 1)**
> Not essential for PCA, but performed when the scale of each feature is very different:
> $\qquad \qquad \qquad \qquad \text{standardized } \tilde{X} = \dfrac{X_{ij}-\mu_j}{\sigma_j}$
> Vector expression: $X^{std} = (X - \vec{\mu}) \oslash \vec{\sigma}$
> $\qquad \qquad \qquad \qquad \vec{\mu} \in \mathbb{R}^{1 \times p} :$ mean vector
> $\qquad \qquad \qquad \qquad \vec{\sigma} \in \mathbb{R}^{1 \times p} :$ standard deviation vector
> $\qquad \qquad \qquad \qquad \oslash :$ broadcasting operation that divides column-wise (element-wise division)
>
> - When standardizing, now in [2] we perform eigendecomposition of the **correlation matrix rather than the covariance matrix**.
> - The principal component directions found are obtained in the same way, but they are simply the maximum variance directions on normalized axes.

### (3) PCA as Linear Transformation

PCA projection: $Z = \tilde{X} W \qquad W = [\vec{v}_1, \dots, \vec{v}_k] \in \mathbb{R}^{p\times k}, \quad Z \in \mathbb{R}^{n\times k}$

$\Rightarrow$ **Therefore, PCA is a linear transformation that passes through the origin and rotates and reduces the basis.**

> Let the transpose of the $i$-th row vector (sample) of $\tilde{X}$ be $\tilde{x}_i$. ($\vec{\tilde{x}_i} \in \mathbb{R}^{p}$ is a $p$-dimensional column vector)
> Also, let $\vec{z}_i$ be the transpose of the $i$-th row vector (sample) of $Z$,
> For each sample $\vec{\tilde{x}_i}$, $\vec{z}_i = W^\top\vec{\tilde{x}_i} \in \mathbb{R}^{k}$ ($k$-dimensional column vector),
> This means PCA is a transformation that sends $\vec{\tilde{x}_i}$ to low-dimensional vector $\vec{z}_i$ by multiplying matrix $W^\top$.

---

## 5. Truncated SVD

Rewriting the original SVD:

**SVD:**

$$
A=U\Sigma V^T
$$

- $A \in \mathbb{R}^{m\times n}$
- $U \in \mathbb{R}^{n\times n}$
- $\Sigma \in \mathbb{R}^{n\times p}$
- $V \in \mathbb{R}^{p\times p}$

Meanwhile, in PCA, we select only the top $k$ principal components from $V=[\vec{v}_1 \quad \vec{v}_2 \quad \cdots \quad \vec{v}_n] \in \mathbb{R}^{p \times p}$. Correspondingly, we can select the top $k$ singular values (=principal components with large information content) from $\Sigma$, and also select only the corresponding first $k$ column vectors from $U$ to rewrite SVD as follows. This is called Truncated SVD.

**Truncated SVD:**

$$
A\approx U_k\Sigma_k V_{k}^T
$$

- $A \in \mathbb{R}^{m\times n}$
- $U_k \in \mathbb{R}^{m\times k}$ : Matrix with only $k$ corresponding column vectors from the front
- $\Sigma_k \in \mathbb{R}^{k\times k}$ : Diagonal matrix with only top $k$ singular values
- $V_k \in \mathbb{R}^{n\times k}$ : Matrix with only $k$ corresponding column vectors from the front

**Therefore, if you're going to do SVD for PCA, you can compute Truncated SVD from the start.**

$$
\tilde{X} \approx U_k \Sigma_k V_{k}^\top \qquad \Rightarrow \qquad Z = \tilde{X} W = \tilde{X} V_k (= U_k \Sigma_k \,\,\, V_{k}^\top V_k) = U_k \Sigma_K
$$

---

_**Source: Lecture Notes from undergraduate and graduate courses - Linear Algebra, Linear Regression, Macroeconometrics, Advanced Machine Learning**_
